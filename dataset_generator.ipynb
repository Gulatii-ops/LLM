{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Generator\n",
        "\n",
        "We will use models from the HuggingFace library to generate text based datasets. We will use gradio to create a user-friendly visualization.\n",
        "\n",
        "This implementation requires a GPU. Therefore for users who do not have a GPU based system, I recommend running this program on Google Colab, with google's free T4 GPU. \n",
        "\n",
        "**Note:** Create a HuggingFace account and create a token key. Add this key as 'hf_token' to your Google Colab keys and give key access to the notebook. For users with GPU based systems, add the HuggingFace key to the system environment.\n",
        "\n",
        "Huggingface models- https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Package Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p13-HfOZGnGp"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch bitsandbytes transformers sentencepiece accelerate gradio huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4A3RlVaGpVQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown\n",
        "import torch\n",
        "import gc\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "from google.colab import userdata\n",
        "from pprint import pprint\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Huggingface token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AmovoWLGvGs"
      },
      "outputs": [],
      "source": [
        "# For Google Colab users\n",
        "hf_token = userdata.get('hf_token')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# For PCs with GPU\n",
        "'''\n",
        "load_dotenv(override=True)\n",
        "hf_token = os.getenv('hf_token')\n",
        "if hf_token:\n",
        "    print(f\"HuggingFace Key exists.\")\n",
        "else:\n",
        "    print(\"HuggingFace Key not set.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Models from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fw6YAJqHlV7"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"llama\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"mistral\" : \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    \"qwen\" : \"Qwen/Qwen3-8B\",\n",
        "    \"gemma\": \"google/gemma-3-1b-it\",\n",
        "    \"huggingface\": \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to generate prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18qMUbYpHs91"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(topic, q1, a1, q2, a2, q3, a3, output_size):\n",
        "    multi_shot_examples = [\n",
        "        {\"instruction\": q1, \"response\": a1},\n",
        "        {\"instruction\": q2, \"response\": a2},\n",
        "        {\"instruction\": q3, \"response\": a3},\n",
        "    ]\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\", \"content\": f\"You are a dataset generator, which is purely text based. You are going to generate text on the topic: {topic} Here are a few examples : {multi_shot_examples}. Generate exactly {output_size} outputs. Output format should be same as the provided examples. Do not include the example prompts in the responses. Do not repeat the responses.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": f\"Generate dataset for the topic {topic} \"\n",
        "        }\n",
        "    ]\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQu353ra8DCv",
        "outputId": "045293c7-b576-44ff-a082-96d4f4278896"
      },
      "outputs": [],
      "source": [
        "m = generate_prompt(default_topic, q_1, a_1, q_2, a_2, q_3, a_3, default_number_of_data)\n",
        "pprint(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantization Config - this allows us to load the model into memory and use less memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG9H6Nr1HuI1"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to define model and stream output\n",
        "P.s. The streamer works only in the cell-based execution of the Jupyter notebook, and not in Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DBhp43bHvd9"
      },
      "outputs": [],
      "source": [
        "def Model(default_topic, q1, a1, q2, a2, q3, a3, default_number_of_data, model_id):\n",
        "    # Generate message\n",
        "    messages = generate_prompt(default_topic, q1, a1, q2, a2, q3, a3, default_number_of_data)\n",
        "    model = models.get(model_id)\n",
        "    # Generate tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Tokenize inputs\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "    # Quantize the model\n",
        "    model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "    # Generate output\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    del model, inputs, tokenizer, outputs, streamer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    split_marker = \"Do not repeat the responses.\"\n",
        "    if split_marker in decoded_output:\n",
        "        decoded_output = decoded_output.split(split_marker, 1)[-1].strip()\n",
        "    return decoded_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt example set 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJUtOMtIHw38"
      },
      "outputs": [],
      "source": [
        "default_topic = \"Fake News vs. Real News. Goal: Generate examples of plausible real and fake headlines. Label them as real or fake, with fact based explanation.\"\n",
        "default_number_of_data = 10\n",
        "q_1 = \"Headline: NASA Confirms Discovery of Liquid Water on Mars.\"\n",
        "a_1 =\"Label: Real. Explanation: This is a verified news story from NASA, first announced in 2015, based on evidence from satellite imaging.\"\n",
        "q_2 = \"Headline: Scientists Reveal That Bananas Can Cure COVID-19.\",\n",
        "a_2 = \"Label: Fake. Explanation: There is no such scientific evidence. This is a typical example of misinformation.\"\n",
        "q_3 = \"Headline: Elon Musk to Launch First Tesla-Branded Country in the Pacific Ocean.\",\n",
        "a_3 = \"Label: Fake. Explanation: This headline sounds sensational and implausible. No credible news source has reported such an event; it's a satirical or fabricated claim.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt example set 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pe10zZhtfDG"
      },
      "outputs": [],
      "source": [
        "default_topic = \"Python programming questions.\"\n",
        "default_number_of_data = 10\n",
        "q_1 = \"How to reverse a string in Python?\"\n",
        "a_1 =\"result = reversed(str) or result = str[::-1] \"\n",
        "q_2 = \"How do you find the product of an array of integers except the integer itself?\",\n",
        "a_2 = \"result = [math.prod(arr[:i] + arr[i+1:]) for i in range(len(arr))]\"\n",
        "q_3 = \"How do you calculate the factorial of a number?\",\n",
        "a_3 = \"result = math.factorial(n)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TpoSVuxKgyC"
      },
      "outputs": [],
      "source": [
        "Model(default_topic, q_1, a_1, q_2, a_2, q_3, a_3, 5, \"huggingface\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXoc7Atng0K"
      },
      "source": [
        "### Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "rBxbSpG7gHlR",
        "outputId": "138bf1c2-2a10-481f-bedd-3d5d870e75b5"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as ui:\n",
        "  with gr.Row():\n",
        "    gr.Markdown(\"# Dataset Generator\")\n",
        "  with gr.Row():\n",
        "    gr.Markdown(\"### This is a Dataset Generation app. You can begin by entering the Topic of your dataset. Please provide 3 examples showcasing the kind of dataset you want. The dataset will be produced in the same format as the provided examples. You can choose a model of your choice. Have fun exploring ...\")\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=1):\n",
        "      topic = gr.Textbox(label= \"Topic (5-10 words):\", value = default_topic)\n",
        "      with gr.Row():\n",
        "        q1 = gr.Textbox(label= \"Instruction 1:\", value=q_1)\n",
        "      with gr.Row():\n",
        "        a1 = gr.Textbox(label= \"Response 1:\", value=a_1)\n",
        "      with gr.Row():\n",
        "        q2 = gr.Textbox(label= \"Instruction 2:\", value=q_2)\n",
        "      with gr.Row():\n",
        "        a2 = gr.Textbox(label= \"Response 2:\", value=a_2)\n",
        "      with gr.Row():\n",
        "        q3 = gr.Textbox(label= \"Instruction 3:\", value=q_3)\n",
        "      with gr.Row():\n",
        "        a3 = gr.Textbox(label= \"Response 3:\", value=a_3)\n",
        "      with gr.Row():\n",
        "        default_number_of_data = gr.Dropdown([1,2,3,4,5,6,7,8,9,10])\n",
        "      with gr.Row():\n",
        "        model = gr.Dropdown([\"mistral\", \"llama\", \"qwen\", \"gemma\", \"huggingface\"], label=\"Select model\", value=\"gemma\")\n",
        "      with gr.Row():\n",
        "        generate_button = gr.Button(\"Generate Dataset\")\n",
        "      with gr.Row():\n",
        "        clear_button= gr.Button(\"Clear\")\n",
        "    with gr.Column(scale=1):\n",
        "      dataset_output = gr.Markdown(\"### Generated Dataset will appear here...\", height=1000)\n",
        "\n",
        "  generate_button.click(Model, inputs=[topic, q1, a1, q2, a2, q3, a3, default_number_of_data, model], outputs=dataset_output)\n",
        "\n",
        "  clear_button.click(inputs=[topic, q1, a1, q2, a2, q3, a3, default_number_of_data, model], outputs=dataset_output)\n",
        "\n",
        "ui.launch(inbrowser=True, share=True)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
